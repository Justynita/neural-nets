{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250c3bf3-1fb7-45f0-a1d3-641103854c8f",
   "metadata": {},
   "source": [
    "## ***Nth-order Polynomial Linear Regression: An Introduction to Neural Networks*** ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8af21-925d-45ad-a168-6c64769a158b",
   "metadata": {},
   "source": [
    "#### ***1. Motivation*** ####\n",
    "\n",
    "Linear regression lies at the heart of neural networks. Although not even close to the pinnacle of neural networks' abilities, linear models are straightforward to analyze and can be studied as a starting point for understanding more complex, non-linear models. Fitting nth-order polynomials with linear regression can be used to introduce basic terminology and fundamental concepts in the field of deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af6506-5c47-499b-b0b0-9c3218a38ddb",
   "metadata": {},
   "source": [
    "\n",
    "#### ***2. Linear Regression*** ####\n",
    "\n",
    "##### *2.1 Preliminaries on Fitting Polynomials* #####\n",
    "\n",
    "An nth-order polynomial can be described as a linear combination of the powers of the independent variable from 0 to $n$. For example, the third-order polynomial\n",
    "\n",
    "$$ y(x) = 1 + x + 3x^2 + 4x^3 $$\n",
    "\n",
    "is described by the linear combination of the coefficients $(1, 1, 3, 4)$ multiplying the powers of the independent variable, $x$. In simple terms, fitting the polynomial with linear regression means estimating the coefficients. Knowing the order $n$ and ordered pairs $(x, y)$ of the polynomial, one could first randomly guess the coefficients and calculate the $y$-values that they produce. Based on the errors between the calculated and actual $y$-values, the coefficients could then be systematically corrected to yield a more accurate estimate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a0fb7-63a7-4230-b68a-ecaaaf130938",
   "metadata": {},
   "source": [
    "\n",
    "##### *2.2 Applicable Terminology in Neural Networks* #####\n",
    "\n",
    "One characteristic of a neural network is the number of *layers*. The first layer is the *input layer*, which for polynomial fitting contains $x$-values from the domain. The last layer is the *output layer*, which contains *predicted* $\\hat{y}$-values. Neural networks can have any number of *hidden layers* in between the input and output layers, but linear regression requires none and for the sake of simplicity, implementing hidden layers is for now omitted. \n",
    "\n",
    "Inter-layer computations must take the inputs from one layer to another. This process is described as *forward propagation*. For this example of linear regression, computations made on $x$-values from the input layer result in $\\hat{y}$-values at the output layer. Here, the powers of the inputs are multiplied by the linear combination of the coefficients to get the outputs. In neural networks terminology, the coefficients for the powers of $x$ from $1$ to $n$ are the *weights*, and the coefficient multiplying $x^0$, or $1$, is the *bias*. For nth-order polynomial fitting with coefficients $\\theta_j$ , the predicted outputs are calculated as \n",
    "\n",
    "$$ \\hat{y_i} = \\sum_{j=0}^n \\theta_j \\cdot x_i^j $$\n",
    "\n",
    "where $i$ goes from $0$ to $N$, the number of input-output pairs $(x_i, y_i)$ training the neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491d697-fced-4f08-842c-1f030035d92e",
   "metadata": {},
   "source": [
    "\n",
    "##### *2.3 Gradient Descent and Mean Squared Error* #####\n",
    "\n",
    "The intuitive approach of randomly initializing and systematically optimizing the weights and biases is implemented in many neural networks. The optimization occurs through *backpropagation*, as the error calculated at the output layer is propagated back to the weights that generated it. One prominent *cost* function for calculating the error is\n",
    "\n",
    "$$ MSE = \\frac{1}{2N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "where the factor of $1/2$ is not necessary but is often included to simplify further calculations. Gradient descent is the algorithm used to update the weights and bias ($\\theta_0$) accordig to\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j} $$\n",
    "\n",
    "where $J$ is the cost function and $\\alpha$ is the *learning rate*, often starting at $0.01$, but whose value depends on the model and can range from $0.0$ to $1.0$. When fitting polynomials, $j$ goes from $0$ to $n$ such that all the coefficients are updated to minimize the error. Since $\\hat{y}$ depends on the weights, chain rule is applied when computing the gradient. For the simple example here, the gradient for each $\\theta_j$ is calculated as \n",
    "\n",
    "$$  \\frac{1}{N} \\sum_{i=1}^N(y_i - \\hat{y_i}) \\cdot x_i^j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ffc7c-efd6-476b-bbae-ba73dd1d998d",
   "metadata": {},
   "source": [
    "\n",
    "#### ***3. Numpy Linear Regressor*** ####\n",
    "\n",
    "The model implemented below uses previously discussed concepts to fit nth-order polynomials. The inputs are initialized in a one-dimensional array, and the outputs are an nth-order polynomial of x. Since the summations above can be simplified using matrix multiplication, the data and weights are stored as matrices for easier computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9b264364-4ba4-49c0-8b52-600040acb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 99: 0.01611062856201498\n",
      "Run 199: 0.00563745743811019\n",
      "Run 299: 0.0019744369242061265\n",
      "Run 399: 0.0006922579862671777\n",
      "Run 499: 0.00024302244943860706\n",
      "Run 599: 8.544425265981266e-05\n",
      "Run 699: 3.009532041193852e-05\n",
      "Run 799: 1.062271419161289e-05\n",
      "Run 899: 3.758841183008195e-06\n",
      "Run 999: 1.3339450980219476e-06\n",
      "\n",
      "Epoch 1: [[0.99920197 1.00209686 3.00013513 3.99970702]]\n",
      "\n",
      "Run 99: 4.7499985397807514e-07\n",
      "Run 199: 1.6980423917039367e-07\n",
      "Run 299: 6.097489724335581e-08\n",
      "Run 399: 2.2007225843138103e-08\n",
      "Run 499: 7.988509007589245e-09\n",
      "Run 599: 2.9182947151672223e-09\n",
      "Run 699: 1.0735551567461952e-09\n",
      "Run 799: 3.979241298820601e-10\n",
      "Run 899: 1.4868704391301974e-10\n",
      "Run 999: 5.6029152797107894e-11\n",
      "\n",
      "Epoch 2: [[0.99998951 1.00001076 3.00000178 3.9999985 ]]\n",
      "\n",
      "Run 99: 2.1297977742968316e-11\n",
      "Run 199: 8.167664285701047e-12\n",
      "Run 299: 3.1599291236471135e-12\n",
      "Run 399: 1.2330920908022253e-12\n",
      "Run 499: 4.851886812599511e-13\n",
      "Run 599: 1.9241094845364487e-13\n",
      "Run 699: 7.68635187755376e-14\n",
      "Run 799: 3.0911682571600885e-14\n",
      "Run 899: 1.2507287151916285e-14\n",
      "Run 999: 5.088207147407031e-15\n",
      "\n",
      "Epoch 3: [[0.99999986 1.00000006 3.00000002 3.99999999]]\n",
      "\n",
      "Run 99: 2.0799692298149832e-15\n",
      "Run 199: 8.538509555310783e-16\n",
      "Run 299: 3.5180415968579464e-16\n",
      "Run 399: 1.4541038276125317e-16\n",
      "Run 499: 6.026561126883956e-17\n",
      "Run 599: 2.5035135337255537e-17\n",
      "Run 699: 1.042040353134914e-17\n",
      "Run 799: 4.3445175886235794e-18\n",
      "Run 899: 1.8138745751611946e-18\n",
      "Run 999: 7.582020430306996e-19\n",
      "\n",
      "Epoch 4: [[1. 1. 3. 4.]]\n",
      "\n",
      "Run 99: 3.1724245822981385e-19\n",
      "Run 199: 1.3284825277702103e-19\n",
      "Run 299: 5.566997051469881e-20\n",
      "Run 399: 2.3341935652072706e-20\n",
      "Run 499: 9.791754560429915e-21\n",
      "Run 599: 4.109234473710199e-21\n",
      "Run 699: 1.725033738703228e-21\n",
      "Run 799: 7.243484939590791e-22\n",
      "Run 899: 3.0424495807805783e-22\n",
      "Run 999: 1.2781381053447497e-22\n",
      "\n",
      "Epoch 5: [[1. 1. 3. 4.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, x, y, n):\n",
    "        self.y = y\n",
    "        self.n = n\n",
    "        self.input_size = len(x)\n",
    "\n",
    "        # (n+1) x (len(x)) matrix with x^0, x^1, ..., x^n\n",
    "        self.x_matrix = np.ones(self.input_size)\n",
    "        for i in range(1, self.n+1):\n",
    "            self.x_matrix = np.vstack((self.x_matrix, np.power(x, i)))\n",
    "\n",
    "        # 1 x (n+1) matrix with weights (weights[0] is the bias)\n",
    "        rng = np.random.default_rng()\n",
    "        self.weights = rng.random((1, self.n+1))\n",
    "\n",
    "    def train(self, num_epochs=5, lr=0.01, t=1000):\n",
    "        for epoch in range(num_epochs):\n",
    "            for run in range(t):\n",
    "                # compute predicted values based on current weights\n",
    "                y_pred = np.matmul(self.weights, self.x_matrix)\n",
    "                # backpropagate the errors\n",
    "                e = y_pred - self.y\n",
    "                if run % 100 == 99:\n",
    "                    # log MSE every 100 runs\n",
    "                    print(f\"Run {run}: {1/(2*self.input_size)*np.dot(e[0,:], e[0,:])}\")\n",
    "                grad_y_pred = 1/self.input_size*e\n",
    "                self.weights -= lr*np.matmul(grad_y_pred, np.transpose(self.x_matrix))\n",
    "            # log the final weights\n",
    "            print(f\"\\nEpoch {epoch+1}: {self.weights}\\n\")\n",
    "\n",
    "''' third-order example '''\n",
    "x = np.linspace(-3.14, 3.14, 100)\n",
    "y = 1 + x + 3*x**2 + 4*x**3\n",
    "\n",
    "third_order = Model(x, y, 3)\n",
    "third_order.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed42179-ec28-4d86-8e64-ee95b181ead2",
   "metadata": {},
   "source": [
    "\n",
    "#### ***4. Taking It Further: Hidden Layers*** ####\n",
    "\n",
    "The above example can hardly be called a neural network, as it has no hidden layers! Adding hidden layers involves making several updates to the model. In the same way that weights in the previous example have been used to propagate the input to the output, more weights can be used to propagate the input through a network of hidden layers to reach the output. However, simply adding more layers does not add much value to the model, as the computations would still be linear. In order to create more complex models, a non-linear *activation function* is applied to each hidden layer. Implementing hidden layers requires additional care during the backpropagation steps, as the layers before and after activation as well as the derivative of the activation function must be taken into account when computing the gradient. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
