{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f048eae6-ae8b-4e28-92da-9c6ea2578706",
   "metadata": {},
   "source": [
    "\n",
    "## ***Multilayer Perceptrons from Scratch*** ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c54c6d-47fd-453f-a636-a84ee90cf54b",
   "metadata": {},
   "source": [
    "\n",
    "#### ***1. Motivation*** ####\n",
    "\n",
    "Using a linear regression model as a starting point, a non-linear neural network can be implemented. Non-linear models can identify more complex patterns than linear models, which makes these neural networks more useful for regression tasks. By introducing perceptrons, which require an activation function, one can implement a multilayer neural network from scratch that can be used to solve complex regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5eca4d-983b-4a18-af4e-594232209567",
   "metadata": {},
   "source": [
    "\n",
    "#### ***2. Introducing Perceptrons*** ####\n",
    "\n",
    "##### *2.1 Extension of Linear Regression* #####\n",
    "\n",
    "A linear regression model can be extended into a non-linear model by introducing hidden layers with *perceptrons* as the nodes. Between the input and output layers, there can be any number of interconnected hidden layers in a neural network. In a *feedfoward neural network*, the nodes are connected without loops, which makes for a relatively simple model to implement from scratch. Data flow and are transformed from one layer to another by computations that include multiplying by the model's weights and adding a bias term. An *activation function* is applied to nodes in each hidden layer to make the computations non-linear. \n",
    "\n",
    "The Model class below is used to implement the neural network. The layers array includes the number of nodes in each layer of the neural network, including the input, hidden, and output layers. Random weights and biases are initialized in their own arrays of matrices that connect neighboring layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "454864d3-f0a3-42ce-8981-95ef34adf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utilities as util\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # initialize random weights and biases\n",
    "        rng = np.random.default_rng()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.weights.append(rng.random((layers[i], layers[i+1])))\n",
    "            self.biases.append(rng.random((1, layers[i+1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b394bc9-044e-459a-8b75-4ad261181553",
   "metadata": {},
   "source": [
    "\n",
    "##### *2.2 Activation Functions: The Non-Linearity of Perceptrons* #####\n",
    "\n",
    "Out of the many activation functions that can be applied to perceptrons, the activation function of choice for this neural network is the simple but effective ReLU, which stands for Rectified Linear Unit. Interestingly, as the name suggests, the plot of the function roughly resembles the current-voltage curve of non-linear devices, or rectifiers. ReLU is piecewise linear, and when the input is less than zero, the output is zero, otherwise the output equals the input. This activation function is especially great to work with due to its derivative, which is either zero or one and is often defined to be zero at the breakpoint. \n",
    "\n",
    "The activation function that will be applied to the hidden layers is implemented below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3ef7dae-3609-4a43-8194-d813b5ee8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(A):\n",
    "    zeros = np.zeros(A.shape)\n",
    "    return np.maximum(A, zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b1218-da3f-4132-bc91-52a9edc937f6",
   "metadata": {},
   "source": [
    "\n",
    "#### ***3. Training the Neural Network*** ####\n",
    "\n",
    "##### *3.1 Predicting the Outputs* #####\n",
    "\n",
    "Forward propagation between layers is an extension of linear regression, which includes multiplying data by weights and adding a bias, followed by activation. In some cases, an activation function may be applied to the output layer to constrain the outputs, but not always. In this neural network, the outputs can be any numerical values, and ReLU is applied only to the hidden layers. The method below is added to the Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6783327e-21de-4c5e-89ae-914d812fe8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@util.add_to_class(Model)\n",
    "def predict(self, input, save_activated=None):\n",
    "    \n",
    "        last_predicted = input.copy()\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            b_matrix = np.tile((self.biases[i]), (len(last_predicted), 1))\n",
    "            last_predicted = np.array(np.matmul(last_predicted, self.weights[i]) + b_matrix)\n",
    "            \n",
    "            if i is not (len(self.layers) - 2):\n",
    "                last_predicted = relu(last_predicted)\n",
    "                \n",
    "            if save_activated is not None:\n",
    "                save_activated.append(last_predicted)\n",
    "\n",
    "        return last_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fdea56-189c-4255-804d-7edf90e30e8d",
   "metadata": {},
   "source": [
    "\n",
    "It should be noted that gradient descent will be used to update the model's weights in a backpropagation process. Thus, as they are computed, the activated layers are saved in an array for later use. The last predicted layer contains the predicted outputs, $\\hat{y}$, which will be used to calculate the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef4774-a7dc-4a6e-b79d-6fef933b2464",
   "metadata": {},
   "source": [
    "\n",
    "##### *3.2 The Objective Function: Introducing Regularization with the Frobenius Norm* #####\n",
    "\n",
    "Similarly to the linear regression example, the loss function, $L$, for this model is MSE. However, in order to prevent overfitting, a *regularization term* \n",
    "\n",
    "$$ s = \\frac{\\lambda}{2} \\sum_{i} \\lVert \\theta_i \\rVert_F^2 $$ \n",
    "\n",
    "is added, where $\\lambda$ is a weight decay parameter, usually between $0.0$ and $0.1$, and the sum of the squared Frobenius norms of the weight matrices is taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3080f526-b7a8-4c07-86b5-19b9eb76c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frob_squared(A):\n",
    "    return np.trace(np.matmul(np.transpose(A), A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82767e27-5d0a-4703-8f45-59396c98cb09",
   "metadata": {},
   "source": [
    "In this neural network, the *objective function*\n",
    "\n",
    "$$ J = L + s $$\n",
    "\n",
    "is the function to be minimized through gradient descent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967e8eb-3e59-4e4f-80ab-ba841b447df9",
   "metadata": {},
   "source": [
    "##### *3.3 Backpropagating the Gradient* #####\n",
    "\n",
    "Gradient descent - the same algorithm used for optimization of coefficients in linear regression - is used to update the weights. \n",
    "Due to hidden layers which undergo activation, the computation of the gradient has more steps than previously discussed for linear regression. The gradient $ \\partial J/\\partial \\theta_i $ for each weight matrix $\\theta_i$ can be computed by applying chain rule to the hidden layer outputs and the derivative of the activation function. A brief walk-through of essential derivations is provided below. \n",
    "\n",
    "Both $\\partial J/\\partial L$ and $\\partial J/\\partial s$ are $1$. Further computations of $\\partial L/\\partial \\theta_i$ and $\\partial s/\\partial \\theta_i$ need to be made. \n",
    "\n",
    "Starting at the output layer and substituting $\\hat{y} = g(z^{(f)})$, where $g$ is the activation function and $z^{(f)}$ contains the values from the last hidden layer, the gradiet due to loss can be computed as\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_i} = \\frac{1}{N} \\sum_{j=1}^N (y_j - g(z_{j}^{(f)})) \\cdot \\frac{\\partial g(z^{(f)})}{\\partial \\theta_i} $$\n",
    "\n",
    "When computing the derivatives using chain rule, the derivative of the activation function, such as ReLU, is multiplied by the derivative of the layer being activated. This expands the computation to \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_i} =  \\frac{1}{N} \\sum_{j=1}^N (y_j - g(z_{j}^{(f)})) \\cdot g^\\prime(a_{j}^{(i-1)} \\theta_i + b_i) \\cdot a^{(i-1)} $$\n",
    "\n",
    "The entire summation term can be called $\\Delta \\theta_i$ for later reference. When computing the gradients for earlier layers, the same approach of applying chain rule can be taken. \n",
    "\n",
    "The gradient from the regularization term can be computed as\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial s}{\\partial \\theta_i} = \\lambda \\theta_i $$\n",
    "\n",
    "Combining the gradients due to loss and regularization results in the following updates to the weights and biases:\n",
    "\n",
    "$$ \\theta_i = \\theta_i - \\alpha [ (\\frac{1}{N} \\Delta \\theta_i) + \\lambda \\theta_i]$$\n",
    "\n",
    "$$ b_i = b_i - \\alpha [\\frac{1}{N} \\Delta b_i] $$\n",
    "\n",
    "\n",
    "where $\\alpha$ is the learning rate and $N$ is the number of samples training the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc849117-f855-487f-84c2-99cece9bba79",
   "metadata": {},
   "source": [
    "##### *3.4 Putting It All Together* #####\n",
    "\n",
    "Training consists of predicting the outputs, calculating the errors, and updating the weights and biases of the model. The function below implements training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5bb74087-753e-4dcf-a38b-1f984b228914",
   "metadata": {},
   "outputs": [],
   "source": [
    "@util.add_to_class(Model)\n",
    "def train(self, inputs, outputs, num_epochs=5, lr=0.01, t=1000):\n",
    "    num_samples = len(inputs)\n",
    "    l = 1e-5 # weight decay parameter\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for run in range(t):\n",
    "            # reset the activated layers and sum of squared Frobenius norms of weights matrices\n",
    "            s = 0\n",
    "            activated = []\n",
    "\n",
    "            # predict the outputs\n",
    "            y_hat = self.predict(inputs, activated)\n",
    "    \n",
    "            # compute the errors and objective\n",
    "            errors = y_hat - outputs\n",
    "            loss = 1/2*(np.dot(errors[:,0], errors[:,0])**2)\n",
    "    \n",
    "            for theta in self.weights:\n",
    "                s += frob_squared(theta)\n",
    "    \n",
    "            objective = 1/num_samples*loss + l/2*s\n",
    "    \n",
    "            # log the objective every 100 runs\n",
    "            if run % 100 == 99:\n",
    "                print(f\"{run}: {objective}\")\n",
    "    \n",
    "            # compute z primes\n",
    "            deltas = self.weights.copy()\n",
    "            z_primes = activated.copy()\n",
    "            z_primes[len(activated) - 1] = errors\n",
    "    \n",
    "            for j in range(len(self.layers) - 2):\n",
    "                a_primes = np.array(activated[len(self.layers) - 3 - j].copy())\n",
    "                a_primes[a_primes > 0] = 1.0\n",
    "                prev_z_prime = (np.multiply(\n",
    "                                    np.matmul(z_primes[len(activated)-1-j], np.transpose(self.weights[len(self.layers)-2-j])), \n",
    "                                    a_primes))\n",
    "                z_primes[len(activated) - 2 - j] = prev_z_prime\n",
    "    \n",
    "            # compute deltas\n",
    "            deltas[0] = (np.matmul(np.transpose(inputs), z_primes[0]))\n",
    "            for j in range(len(self.layers) - 3):\n",
    "                deltas[j+1] = (np.matmul(np.transpose(activated[j]), z_primes[j+1]))\n",
    "    \n",
    "            # update weights and biases\n",
    "            for j in range(len(self.weights) - 1):\n",
    "                self.weights[j] = self.weights[j] - lr*(1/num_samples*deltas[j] + l*self.weights[j])\n",
    "                self.biases[j] = self.biases[j] - lr/num_samples*np.sum(z_primes[j], axis=0)\n",
    "                \n",
    "        print(f\"\\nEpoch {epoch+1}: {y_hat}\\n\")\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04ae02-60f4-478f-9b5e-962f214188bc",
   "metadata": {},
   "source": [
    "\n",
    "##### 3.5 Neural Net Example #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ee96fc0f-7f53-48db-b6b2-27fb8907f939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 6.085642574048144\n",
      "199: 0.33351983259576096\n",
      "299: 0.02866613721719484\n",
      "399: 0.003720760844375917\n",
      "499: 0.0012717620825654645\n",
      "599: 0.0008248930180855524\n",
      "699: 0.0007021940144321414\n",
      "799: 0.000645732883272914\n",
      "899: 0.0006172330887271987\n",
      "999: 0.0006028268417983063\n",
      "\n",
      "Epoch 1: [[ 1.23982529]\n",
      " [ 1.87909969]\n",
      " [11.94381113]\n",
      " [ 1.62871017]\n",
      " [ 7.17593281]\n",
      " [ 4.62564826]]\n",
      "\n",
      "99: 0.0005896939678572123\n",
      "199: 0.0005770686259332289\n",
      "299: 0.0005647949749726724\n",
      "399: 0.0005527917163813295\n",
      "499: 0.0005410511722633253\n",
      "599: 0.0005295256610297155\n",
      "699: 0.0005182590404130314\n",
      "799: 0.000507209099022288\n",
      "899: 0.000496411012342344\n",
      "999: 0.0004858682510294627\n",
      "\n",
      "Epoch 2: [[ 1.24478915]\n",
      " [ 1.89668041]\n",
      " [11.94733614]\n",
      " [ 1.61272036]\n",
      " [ 7.17220554]\n",
      " [ 4.62427663]]\n",
      "\n",
      "99: 0.0004754911299646224\n",
      "199: 0.0004653684304947161\n",
      "299: 0.00045548075313369134\n",
      "399: 0.00044580734991361604\n",
      "499: 0.0004363504369141104\n",
      "599: 0.0004271139905374439\n",
      "699: 0.0004142914811181046\n",
      "799: 0.00040091688229794653\n",
      "899: 0.0003877727691739443\n",
      "999: 0.00037456095743443736\n",
      "\n",
      "Epoch 3: [[ 1.24701344]\n",
      " [ 1.91651781]\n",
      " [11.95027499]\n",
      " [ 1.5970776 ]\n",
      " [ 7.16408056]\n",
      " [ 4.62430962]]\n",
      "\n",
      "99: 0.0003629417440342827\n",
      "199: 0.00035453508533496924\n",
      "299: 0.00034629257932786606\n",
      "399: 0.0003382337378226731\n",
      "499: 0.00033035661981632307\n",
      "599: 0.00032263785003278995\n",
      "699: 0.00031505787982418474\n",
      "799: 0.0003076649621866087\n",
      "899: 0.00030041266039545426\n",
      "999: 0.0002933528512711113\n",
      "\n",
      "Epoch 4: [[ 1.24750409]\n",
      " [ 1.93236283]\n",
      " [11.9541664 ]\n",
      " [ 1.5822034 ]\n",
      " [ 7.15609985]\n",
      " [ 4.62647815]]\n",
      "\n",
      "99: 0.0002864320826412571\n",
      "199: 0.00027964667969369503\n",
      "299: 0.00027299285882765554\n",
      "399: 0.0002665382972302978\n",
      "499: 0.000260215843946758\n",
      "599: 0.00025402258426369994\n",
      "699: 0.0002479560030478937\n",
      "799: 0.00024201321458656308\n",
      "899: 0.00023625906405504943\n",
      "999: 0.00023058114543961808\n",
      "\n",
      "Epoch 5: [[ 1.24922284]\n",
      " [ 1.9457968 ]\n",
      " [11.95781937]\n",
      " [ 1.56940123]\n",
      " [ 7.14686172]\n",
      " [ 4.63053306]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = (2, 3, 5, 2, 1)\n",
    "data = np.array([[1, 0.5], [1, 1], [3, 3], [0.5, 1], [3, 2], [2.5, 1.5]])\n",
    "outcomes = np.array([[1.25], [2], [12], [1.5], [7], [4.75]])   # first + second^2\n",
    "\n",
    "neural_net = Model(layers)\n",
    "predicted = neural_net.train(data, outcomes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
